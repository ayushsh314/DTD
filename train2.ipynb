{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for transformation\n",
    "\n",
    "import torch # PyTorch package\n",
    "import torchvision # load datasets\n",
    "import torchvision.transforms as transforms # transform data\n",
    "import torch.nn as nn # basic building block for neural neteorks\n",
    "import torch.nn.functional as F # import convolution functions like Relu\n",
    "import torch.optim as optim # optimzer\n",
    "import os\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, BatchNorm2d, Dropout\n",
    "# for reading and displaying images\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.functional as fn\n",
    "from torch.utils.data import random_split, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayushsharma/Documents/Programming/Python/dtd_custom'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5540\n",
      "4155\n",
      "1385\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 5540\n",
      "    Root location: /Users/ayushsharma/Documents/Programming/Python/dtd_custom/images/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "dataset = ImageFolder('/Users/ayushsharma/Documents/Programming/Python/dtd_custom/images/', transform=Compose([Resize((224,224)),ToTensor()]))\n",
    "print(len(dataset))\n",
    "datasets = train_val_dataset(dataset)\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "# The original dataset is available in the Subset class\n",
    "print(datasets['train'].dataset)\n",
    "\n",
    "dataloaders = {x:DataLoader(datasets[x],32, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "x,y = next(iter(dataloaders['train']))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6667, 0.6667, 0.6667,  ..., 0.7020, 0.6941, 0.6902],\n",
       "          [0.6706, 0.6706, 0.6706,  ..., 0.7020, 0.6941, 0.6902],\n",
       "          [0.6745, 0.6745, 0.6745,  ..., 0.7020, 0.6941, 0.6902],\n",
       "          ...,\n",
       "          [0.4000, 0.5373, 0.7294,  ..., 0.4275, 0.5137, 0.6078],\n",
       "          [0.3490, 0.4588, 0.6745,  ..., 0.3922, 0.3804, 0.5765],\n",
       "          [0.3529, 0.4235, 0.5922,  ..., 0.3804, 0.3725, 0.4902]],\n",
       "\n",
       "         [[0.6667, 0.6667, 0.6667,  ..., 0.7137, 0.7176, 0.7216],\n",
       "          [0.6706, 0.6706, 0.6706,  ..., 0.7137, 0.7176, 0.7216],\n",
       "          [0.6745, 0.6745, 0.6745,  ..., 0.7098, 0.7137, 0.7176],\n",
       "          ...,\n",
       "          [0.5137, 0.6510, 0.8510,  ..., 0.5137, 0.6196, 0.7137],\n",
       "          [0.4392, 0.5647, 0.7961,  ..., 0.4627, 0.4706, 0.6706],\n",
       "          [0.4196, 0.5216, 0.7176,  ..., 0.4431, 0.4471, 0.5686]],\n",
       "\n",
       "         [[0.6667, 0.6667, 0.6667,  ..., 0.6863, 0.6745, 0.6706],\n",
       "          [0.6706, 0.6706, 0.6706,  ..., 0.6863, 0.6824, 0.6745],\n",
       "          [0.6706, 0.6706, 0.6706,  ..., 0.6902, 0.6863, 0.6824],\n",
       "          ...,\n",
       "          [0.8000, 0.9020, 0.9725,  ..., 0.6118, 0.7333, 0.8392],\n",
       "          [0.7098, 0.8078, 0.9333,  ..., 0.5490, 0.5804, 0.7882],\n",
       "          [0.6784, 0.7529, 0.8588,  ..., 0.5216, 0.5529, 0.6863]]],\n",
       "\n",
       "\n",
       "        [[[0.2824, 0.2706, 0.2784,  ..., 0.2667, 0.2706, 0.2627],\n",
       "          [0.2510, 0.2471, 0.3020,  ..., 0.2745, 0.2667, 0.2549],\n",
       "          [0.2549, 0.2471, 0.2784,  ..., 0.2745, 0.2667, 0.2549],\n",
       "          ...,\n",
       "          [0.3020, 0.2824, 0.3137,  ..., 0.1529, 0.1569, 0.1647],\n",
       "          [0.3294, 0.2745, 0.2588,  ..., 0.1569, 0.1569, 0.1569],\n",
       "          [0.3294, 0.3059, 0.2275,  ..., 0.1647, 0.1608, 0.1451]],\n",
       "\n",
       "         [[0.2980, 0.2863, 0.2941,  ..., 0.2667, 0.2745, 0.2667],\n",
       "          [0.2667, 0.2627, 0.3176,  ..., 0.2745, 0.2706, 0.2588],\n",
       "          [0.2706, 0.2627, 0.2941,  ..., 0.2745, 0.2667, 0.2588],\n",
       "          ...,\n",
       "          [0.2667, 0.2471, 0.2784,  ..., 0.1451, 0.1412, 0.1451],\n",
       "          [0.2941, 0.2392, 0.2235,  ..., 0.1490, 0.1451, 0.1373],\n",
       "          [0.2941, 0.2706, 0.1922,  ..., 0.1569, 0.1451, 0.1255]],\n",
       "\n",
       "         [[0.3333, 0.3216, 0.3294,  ..., 0.2980, 0.2980, 0.2863],\n",
       "          [0.3020, 0.2980, 0.3529,  ..., 0.3059, 0.2941, 0.2784],\n",
       "          [0.3059, 0.2980, 0.3294,  ..., 0.3059, 0.2941, 0.2745],\n",
       "          ...,\n",
       "          [0.2000, 0.1804, 0.2118,  ..., 0.1255, 0.1216, 0.1294],\n",
       "          [0.2275, 0.1725, 0.1569,  ..., 0.1294, 0.1255, 0.1216],\n",
       "          [0.2275, 0.2039, 0.1255,  ..., 0.1373, 0.1255, 0.1098]]],\n",
       "\n",
       "\n",
       "        [[[0.2431, 0.2471, 0.2588,  ..., 0.6745, 0.6549, 0.6510],\n",
       "          [0.2588, 0.2549, 0.2706,  ..., 0.6745, 0.6549, 0.6510],\n",
       "          [0.2667, 0.2627, 0.2824,  ..., 0.6745, 0.6588, 0.6510],\n",
       "          ...,\n",
       "          [0.7294, 0.8471, 0.9412,  ..., 0.1373, 0.1255, 0.1098],\n",
       "          [0.7098, 0.8157, 0.9333,  ..., 0.1373, 0.1255, 0.1137],\n",
       "          [0.6941, 0.7804, 0.8980,  ..., 0.1412, 0.1255, 0.1137]],\n",
       "\n",
       "         [[0.2471, 0.2549, 0.2667,  ..., 0.5922, 0.5686, 0.5647],\n",
       "          [0.2667, 0.2627, 0.2745,  ..., 0.5922, 0.5725, 0.5647],\n",
       "          [0.2745, 0.2667, 0.2784,  ..., 0.5922, 0.5725, 0.5647],\n",
       "          ...,\n",
       "          [0.6471, 0.7725, 0.8745,  ..., 0.0941, 0.0824, 0.0627],\n",
       "          [0.6235, 0.7373, 0.8588,  ..., 0.0941, 0.0824, 0.0667],\n",
       "          [0.6039, 0.6980, 0.8196,  ..., 0.0980, 0.0824, 0.0667]],\n",
       "\n",
       "         [[0.0745, 0.0706, 0.0745,  ..., 0.3882, 0.3765, 0.3765],\n",
       "          [0.0863, 0.0784, 0.0824,  ..., 0.3843, 0.3686, 0.3686],\n",
       "          [0.0902, 0.0824, 0.0902,  ..., 0.3843, 0.3686, 0.3686],\n",
       "          ...,\n",
       "          [0.3333, 0.4392, 0.5216,  ..., 0.0745, 0.0706, 0.0667],\n",
       "          [0.3059, 0.4000, 0.5020,  ..., 0.0745, 0.0706, 0.0627],\n",
       "          [0.2902, 0.3686, 0.4627,  ..., 0.0706, 0.0667, 0.0627]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.7176, 0.7098, 0.7176,  ..., 0.4863, 0.4745, 0.4863],\n",
       "          [0.7020, 0.6627, 0.6157,  ..., 0.4902, 0.4824, 0.4941],\n",
       "          [0.6667, 0.5765, 0.4549,  ..., 0.5059, 0.4902, 0.4980],\n",
       "          ...,\n",
       "          [0.8392, 0.8078, 0.7686,  ..., 0.8392, 0.8510, 0.8667],\n",
       "          [0.8863, 0.8745, 0.8549,  ..., 0.8941, 0.9059, 0.9176],\n",
       "          [0.8980, 0.8980, 0.8902,  ..., 0.9255, 0.9333, 0.9412]],\n",
       "\n",
       "         [[0.7333, 0.7216, 0.7255,  ..., 0.5216, 0.5098, 0.5020],\n",
       "          [0.7098, 0.6667, 0.6157,  ..., 0.5137, 0.5098, 0.5059],\n",
       "          [0.6627, 0.5686, 0.4353,  ..., 0.5137, 0.5059, 0.5020],\n",
       "          ...,\n",
       "          [0.8235, 0.7922, 0.7569,  ..., 0.8118, 0.8392, 0.8627],\n",
       "          [0.8863, 0.8706, 0.8510,  ..., 0.8824, 0.9020, 0.9176],\n",
       "          [0.9059, 0.9020, 0.8980,  ..., 0.9255, 0.9333, 0.9412]],\n",
       "\n",
       "         [[0.7059, 0.6941, 0.7020,  ..., 0.5333, 0.5216, 0.5098],\n",
       "          [0.6784, 0.6353, 0.5843,  ..., 0.5216, 0.5176, 0.5098],\n",
       "          [0.6275, 0.5333, 0.4000,  ..., 0.5059, 0.5059, 0.5020],\n",
       "          ...,\n",
       "          [0.8078, 0.7725, 0.7373,  ..., 0.8039, 0.8314, 0.8588],\n",
       "          [0.8745, 0.8627, 0.8392,  ..., 0.8824, 0.9059, 0.9176],\n",
       "          [0.9059, 0.9059, 0.8941,  ..., 0.9333, 0.9412, 0.9451]]],\n",
       "\n",
       "\n",
       "        [[[0.4196, 0.3686, 0.2667,  ..., 0.3686, 0.3569, 0.3451],\n",
       "          [0.3882, 0.2941, 0.2353,  ..., 0.3529, 0.3686, 0.3608],\n",
       "          [0.3412, 0.2588, 0.1922,  ..., 0.3333, 0.3373, 0.3451],\n",
       "          ...,\n",
       "          [0.8235, 0.8510, 0.9137,  ..., 0.4157, 0.2314, 0.1882],\n",
       "          [0.8431, 0.8863, 0.9608,  ..., 0.3922, 0.1490, 0.0863],\n",
       "          [0.8471, 0.9020, 0.9608,  ..., 0.4588, 0.1529, 0.0902]],\n",
       "\n",
       "         [[0.4392, 0.4039, 0.3137,  ..., 0.3176, 0.3255, 0.3451],\n",
       "          [0.4078, 0.3255, 0.2784,  ..., 0.2902, 0.3216, 0.3451],\n",
       "          [0.3569, 0.2902, 0.2353,  ..., 0.2549, 0.2784, 0.3020],\n",
       "          ...,\n",
       "          [0.7333, 0.7569, 0.8235,  ..., 0.5804, 0.4980, 0.4941],\n",
       "          [0.7412, 0.7765, 0.8431,  ..., 0.6039, 0.4902, 0.4863],\n",
       "          [0.7333, 0.7725, 0.8196,  ..., 0.6627, 0.4980, 0.5020]],\n",
       "\n",
       "         [[0.4157, 0.3961, 0.3176,  ..., 0.2588, 0.2235, 0.1961],\n",
       "          [0.3961, 0.3216, 0.2941,  ..., 0.2353, 0.2431, 0.2275],\n",
       "          [0.3647, 0.2980, 0.2588,  ..., 0.1882, 0.1961, 0.2000],\n",
       "          ...,\n",
       "          [0.4431, 0.4353, 0.4745,  ..., 0.4196, 0.3137, 0.3020],\n",
       "          [0.4510, 0.4627, 0.5098,  ..., 0.4588, 0.3176, 0.3059],\n",
       "          [0.4353, 0.4510, 0.4863,  ..., 0.5373, 0.3373, 0.3373]]],\n",
       "\n",
       "\n",
       "        [[[0.7804, 0.7765, 0.7686,  ..., 0.5137, 0.5176, 0.5216],\n",
       "          [0.7882, 0.7843, 0.7765,  ..., 0.5176, 0.5137, 0.5255],\n",
       "          [0.7843, 0.7804, 0.7725,  ..., 0.5255, 0.5255, 0.5216],\n",
       "          ...,\n",
       "          [0.2510, 0.2471, 0.2471,  ..., 0.6039, 0.5961, 0.5882],\n",
       "          [0.2588, 0.2549, 0.2549,  ..., 0.6000, 0.5922, 0.5882],\n",
       "          [0.2627, 0.2588, 0.2627,  ..., 0.5961, 0.5961, 0.5922]],\n",
       "\n",
       "         [[0.7961, 0.7922, 0.7882,  ..., 0.4431, 0.4471, 0.4471],\n",
       "          [0.8039, 0.8000, 0.7961,  ..., 0.4471, 0.4431, 0.4549],\n",
       "          [0.8000, 0.7961, 0.7922,  ..., 0.4510, 0.4549, 0.4549],\n",
       "          ...,\n",
       "          [0.1922, 0.1882, 0.1882,  ..., 0.6118, 0.6039, 0.6000],\n",
       "          [0.2000, 0.1961, 0.1961,  ..., 0.6157, 0.6078, 0.6039],\n",
       "          [0.2039, 0.2000, 0.2039,  ..., 0.6157, 0.6118, 0.6078]],\n",
       "\n",
       "         [[0.8314, 0.8275, 0.8157,  ..., 0.3961, 0.4000, 0.4039],\n",
       "          [0.8392, 0.8353, 0.8235,  ..., 0.3922, 0.3922, 0.4039],\n",
       "          [0.8353, 0.8314, 0.8196,  ..., 0.3961, 0.4000, 0.3961],\n",
       "          ...,\n",
       "          [0.1804, 0.1765, 0.1765,  ..., 0.6588, 0.6510, 0.6471],\n",
       "          [0.1882, 0.1843, 0.1843,  ..., 0.6588, 0.6510, 0.6471],\n",
       "          [0.1922, 0.1882, 0.1922,  ..., 0.6588, 0.6549, 0.6510]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 35, 24, 39, 25, 25, 33, 42, 14, 11, 14, 29, 32, 29,  3, 11, 20, 38,\n",
       "         0, 10, 37, 10, 30, 27, 43, 37, 29, 44, 16, 39, 36, 13])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(x)\n",
    "train_y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(42)\n",
    "train_x, val_x = random_split(train_x, [0.8,0.2], generator=gen)\n",
    "train_y, val_y = random_split(train_y, [0.8,0.2], generator=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = np.array(val_x)\n",
    "val_y = np.array(val_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.8235294  0.8509804  0.88235295 ... 0.8784314  0.93333334\n",
      "    0.9843137 ]\n",
      "   [0.8039216  0.8745098  0.89411765 ... 0.94509804 0.95686275\n",
      "    0.972549  ]\n",
      "   [0.6862745  0.8392157  0.8666667  ... 0.98039216 0.93333334\n",
      "    0.8980392 ]\n",
      "   ...\n",
      "   [0.5254902  0.5058824  0.5176471  ... 0.78039217 0.8156863\n",
      "    0.78431374]\n",
      "   [0.56078434 0.53333336 0.52156866 ... 0.61960787 0.6627451\n",
      "    0.68235296]\n",
      "   [0.62352943 0.60784316 0.6        ... 0.6509804  0.59607846\n",
      "    0.5882353 ]]\n",
      "\n",
      "  [[0.85490197 0.8862745  0.92156863 ... 0.8627451  0.9254902\n",
      "    0.9843137 ]\n",
      "   [0.8352941  0.90588236 0.92941177 ... 0.93333334 0.9490196\n",
      "    0.9607843 ]\n",
      "   [0.7176471  0.87058824 0.9019608  ... 0.9764706  0.92941177\n",
      "    0.89411765]\n",
      "   ...\n",
      "   [0.6156863  0.6039216  0.61960787 ... 0.8156863  0.84313726\n",
      "    0.8117647 ]\n",
      "   [0.64705884 0.6313726  0.627451   ... 0.6627451  0.7019608\n",
      "    0.72156864]\n",
      "   [0.7058824  0.7019608  0.70980394 ... 0.69803923 0.6392157\n",
      "    0.6313726 ]]\n",
      "\n",
      "  [[0.9411765  0.95686275 0.96862745 ... 0.84705883 0.9098039\n",
      "    0.9647059 ]\n",
      "   [0.9098039  0.972549   0.98039216 ... 0.92156863 0.93333334\n",
      "    0.94509804]\n",
      "   [0.8        0.9411765  0.96862745 ... 0.9607843  0.9137255\n",
      "    0.8784314 ]\n",
      "   ...\n",
      "   [0.7647059  0.7647059  0.78431374 ... 0.9019608  0.9372549\n",
      "    0.9098039 ]\n",
      "   [0.79607844 0.7921569  0.79607844 ... 0.7529412  0.8039216\n",
      "    0.83137256]\n",
      "   [0.85490197 0.8666667  0.8784314  ... 0.7921569  0.7490196\n",
      "    0.74509805]]]\n",
      "\n",
      "\n",
      " [[[0.7490196  0.7019608  0.61960787 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.7137255  0.67058825 0.6039216  ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.8352941  0.68235296 0.5764706  ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [0.9254902  0.9254902  0.92156863 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.92156863 0.92156863 0.91764706 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.92156863 0.91764706 0.91764706 ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[0.5568628  0.5137255  0.44313726 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.54509807 0.5058824  0.44313726 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.6901961  0.53333336 0.43137255 ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [0.827451   0.827451   0.83137256 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.8235294  0.8235294  0.827451   ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.827451   0.827451   0.83137256 ... 1.         1.\n",
      "    1.        ]]\n",
      "\n",
      "  [[0.56078434 0.48235294 0.35686275 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.5137255  0.44705883 0.34509805 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.6117647  0.43529412 0.30588236 ... 1.         1.\n",
      "    1.        ]\n",
      "   ...\n",
      "   [0.7411765  0.7411765  0.7490196  ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.7372549  0.7411765  0.74509805 ... 1.         1.\n",
      "    1.        ]\n",
      "   [0.73333335 0.7372549  0.7411765  ... 1.         1.\n",
      "    1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.9254902  0.9254902  0.9411765  ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.9254902  0.90588236 0.81960785 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.7607843  0.62352943 0.44705883 ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.8039216  0.7921569  0.83137256 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.81960785 0.8117647  0.79607844 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.8235294  0.80784315 0.7882353  ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.88235295 0.8627451  0.8509804  ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.8862745  0.84705883 0.74509805 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.7176471  0.5764706  0.4        ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.73333335 0.72156864 0.7647059  ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.7490196  0.7411765  0.7294118  ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.7529412  0.7411765  0.72156864 ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.85490197 0.85882354 0.8745098  ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.8745098  0.8509804  0.77254903 ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   [0.7294118  0.59607846 0.4392157  ... 0.00784314 0.00784314\n",
      "    0.00784314]\n",
      "   ...\n",
      "   [0.6509804  0.6313726  0.65882355 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.6666667  0.64705884 0.62352943 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.67058825 0.64705884 0.6156863  ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.5882353  0.57254905 0.54509807 ... 0.6431373  0.654902\n",
      "    0.63529414]\n",
      "   [0.5568628  0.5529412  0.5764706  ... 0.6627451  0.6431373\n",
      "    0.654902  ]\n",
      "   [0.53333336 0.5254902  0.50980395 ... 0.6392157  0.6039216\n",
      "    0.65882355]\n",
      "   ...\n",
      "   [0.61960787 0.6        0.654902   ... 0.5019608  0.5254902\n",
      "    0.5254902 ]\n",
      "   [0.5372549  0.56078434 0.654902   ... 0.49803922 0.5372549\n",
      "    0.5058824 ]\n",
      "   [0.4862745  0.54901963 0.5882353  ... 0.5294118  0.5568628\n",
      "    0.54901963]]\n",
      "\n",
      "  [[0.5686275  0.5411765  0.5176471  ... 0.64705884 0.654902\n",
      "    0.6313726 ]\n",
      "   [0.5372549  0.53333336 0.56078434 ... 0.6627451  0.6431373\n",
      "    0.6509804 ]\n",
      "   [0.50980395 0.5137255  0.5137255  ... 0.6392157  0.6\n",
      "    0.65882355]\n",
      "   ...\n",
      "   [0.6313726  0.60784316 0.6666667  ... 0.5058824  0.53333336\n",
      "    0.53333336]\n",
      "   [0.5254902  0.54901963 0.6509804  ... 0.49803922 0.5372549\n",
      "    0.5254902 ]\n",
      "   [0.48235294 0.54901963 0.5921569  ... 0.54509807 0.56078434\n",
      "    0.5529412 ]]\n",
      "\n",
      "  [[0.54901963 0.5294118  0.5176471  ... 0.64705884 0.654902\n",
      "    0.63529414]\n",
      "   [0.5137255  0.52156866 0.5568628  ... 0.6627451  0.6431373\n",
      "    0.6509804 ]\n",
      "   [0.4862745  0.49411765 0.5019608  ... 0.6392157  0.6\n",
      "    0.64705884]\n",
      "   ...\n",
      "   [0.6431373  0.62352943 0.6784314  ... 0.5294118  0.5411765\n",
      "    0.5294118 ]\n",
      "   [0.54509807 0.5686275  0.6666667  ... 0.5176471  0.5529412\n",
      "    0.5372549 ]\n",
      "   [0.49411765 0.5568628  0.6        ... 0.56078434 0.5882353\n",
      "    0.57254905]]]\n",
      "\n",
      "\n",
      " [[[0.47058824 0.4627451  0.4627451  ... 0.21568628 0.21568628\n",
      "    0.22352941]\n",
      "   [0.5372549  0.5411765  0.5254902  ... 0.22352941 0.22352941\n",
      "    0.22745098]\n",
      "   [0.64705884 0.63529414 0.6431373  ... 0.21176471 0.21568628\n",
      "    0.21960784]\n",
      "   ...\n",
      "   [0.7411765  0.7372549  0.7372549  ... 0.78431374 0.7882353\n",
      "    0.7921569 ]\n",
      "   [0.7529412  0.7372549  0.74509805 ... 0.7607843  0.75686276\n",
      "    0.7490196 ]\n",
      "   [0.7529412  0.7411765  0.7490196  ... 0.83137256 0.9098039\n",
      "    0.83137256]]\n",
      "\n",
      "  [[0.2627451  0.2509804  0.25490198 ... 0.20784314 0.21176471\n",
      "    0.21960784]\n",
      "   [0.25490198 0.25490198 0.25490198 ... 0.21176471 0.21960784\n",
      "    0.22352941]\n",
      "   [0.26666668 0.2627451  0.29411766 ... 0.2        0.20784314\n",
      "    0.21568628]\n",
      "   ...\n",
      "   [0.2901961  0.27450982 0.27058825 ... 0.21176471 0.21568628\n",
      "    0.21960784]\n",
      "   [0.3019608  0.27450982 0.26666668 ... 0.21960784 0.21960784\n",
      "    0.21176471]\n",
      "   [0.29803923 0.2784314  0.27058825 ... 0.29411766 0.3764706\n",
      "    0.29411766]]\n",
      "\n",
      "  [[0.16862746 0.18039216 0.21568628 ... 0.23137255 0.24313726\n",
      "    0.2509804 ]\n",
      "   [0.1254902  0.15294118 0.18431373 ... 0.23921569 0.24705882\n",
      "    0.25490198]\n",
      "   [0.10980392 0.12156863 0.18039216 ... 0.22745098 0.23529412\n",
      "    0.23921569]\n",
      "   ...\n",
      "   [0.08627451 0.07843138 0.07843138 ... 0.03137255 0.04313726\n",
      "    0.04705882]\n",
      "   [0.09019608 0.07843138 0.07843138 ... 0.07058824 0.08235294\n",
      "    0.07450981]\n",
      "   [0.09019608 0.07843138 0.08235294 ... 0.15294118 0.24313726\n",
      "    0.16078432]]]\n",
      "\n",
      "\n",
      " [[[0.2901961  0.2901961  0.2901961  ... 0.5411765  0.4862745\n",
      "    0.4       ]\n",
      "   [0.2901961  0.2901961  0.2901961  ... 0.48235294 0.4392157\n",
      "    0.3764706 ]\n",
      "   [0.2901961  0.2901961  0.2901961  ... 0.40392157 0.33333334\n",
      "    0.25490198]\n",
      "   ...\n",
      "   [0.39215687 0.38039216 0.36862746 ... 0.1882353  0.1882353\n",
      "    0.1882353 ]\n",
      "   [0.39607844 0.38431373 0.37254903 ... 0.1882353  0.1882353\n",
      "    0.1882353 ]\n",
      "   [0.4117647  0.4        0.38431373 ... 0.1882353  0.1882353\n",
      "    0.1882353 ]]\n",
      "\n",
      "  [[0.38431373 0.38431373 0.38431373 ... 0.7176471  0.6627451\n",
      "    0.5764706 ]\n",
      "   [0.38431373 0.38431373 0.38431373 ... 0.654902   0.6117647\n",
      "    0.54509807]\n",
      "   [0.38431373 0.38431373 0.38431373 ... 0.5647059  0.49411765\n",
      "    0.41568628]\n",
      "   ...\n",
      "   [0.5647059  0.5686275  0.5686275  ... 0.32156864 0.32156864\n",
      "    0.32156864]\n",
      "   [0.57254905 0.5764706  0.5764706  ... 0.32156864 0.32156864\n",
      "    0.32156864]\n",
      "   [0.5882353  0.5921569  0.5921569  ... 0.32156864 0.32156864\n",
      "    0.32156864]]\n",
      "\n",
      "  [[0.5176471  0.5176471  0.5254902  ... 0.98039216 0.9254902\n",
      "    0.84313726]\n",
      "   [0.5176471  0.5176471  0.5254902  ... 0.9098039  0.8666667\n",
      "    0.8039216 ]\n",
      "   [0.5176471  0.5176471  0.5254902  ... 0.8117647  0.7372549\n",
      "    0.65882355]\n",
      "   ...\n",
      "   [0.9490196  0.9372549  0.93333334 ... 0.57254905 0.57254905\n",
      "    0.57254905]\n",
      "   [0.9372549  0.92941177 0.92941177 ... 0.57254905 0.57254905\n",
      "    0.57254905]\n",
      "   [0.9490196  0.9372549  0.9411765  ... 0.57254905 0.57254905\n",
      "    0.57254905]]]]\n",
      "[16 10 14 37 43 24]\n"
     ]
    }
   ],
   "source": [
    "print(val_x)\n",
    "print(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = np.load(\"train_x.npy\")\n",
    "# train_y = np.load(\"train_y.npy\")\n",
    "# val_x = np.load(\"val_x.npy\")\n",
    "# val_y = np.load(\"val_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_x.reshape(4155, 1, 128, 128)\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "# converting validation images into torch format\n",
    "# val_x = val_x.reshape(1355, 1, 128, 128)\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "# converting the target into torch format\n",
    "val_y = torch.from_numpy(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=254016, out_features=47, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            Conv2d(1, 64, 2),\n",
    "            BatchNorm2d(64),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Dropout(0.4),\n",
    "\n",
    "            # Conv2d(32, 64, 3),\n",
    "            # BatchNorm2d(64),\n",
    "            # ReLU(inplace=True),\n",
    "            # MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Dropout(0.35)\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(64*63*63, 47)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5, weight_decay=5e-6)\n",
    "# defining the loss function for multi class\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(arr1, arr2):\n",
    "    accuracy = np.sum(np.equal(arr1,arr2))/len(arr1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayushsharma/Documents/Programming/Python/dtd_custom'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs to run(ideal 15 to 20): \n",
      "Training started\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 2, 2], expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m \u001b[39m# prediction for training set\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m output_train \u001b[39m=\u001b[39m model(x_train)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m     35\u001b[0m     output_train \u001b[39m=\u001b[39m output_train\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 25\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn_layers(x)\n\u001b[1;32m     26\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_layers(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 2, 2], expected input[1, 3, 224, 224] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "# defining the number of epochs\n",
    "print(\"Number of epochs to run(ideal 15 to 20): \")\n",
    "n_epochs = 10\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "# training the model\n",
    "print(\"Training started\")\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    loss_train = 0\n",
    "    loss_val = 0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(4155):\n",
    "        # getting the training set\n",
    "        x_train, y_train = Variable(train_x[i:i+1]), Variable(train_y[i:i+1]).type(torch.LongTensor)\n",
    "        \n",
    "        # converting the data into GPU format\n",
    "        if torch.cuda.is_available():\n",
    "            x_train = x_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "\n",
    "        # clearing the Gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prediction for training set\n",
    "        output_train = model(x_train)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            output_train = output_train.cuda()\n",
    "\n",
    "        # computing the training loss\n",
    "        loss = criterion(output_train, y_train)\n",
    "        loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        softmax = torch.exp(output_train).cpu()\n",
    "        prob = list(softmax.detach().numpy())\n",
    "        predictions.append(np.argmax(prob, axis=1).item())\n",
    "\n",
    "        \n",
    "    for i in range(1385):\n",
    "        # # prediction for validation set\n",
    "        x_val, y_val = Variable(val_x[i:i+1]), Variable(val_y[i:i+1]).type(torch.LongTensor)\n",
    "        if torch.cuda.is_available():\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "        \n",
    "        output_val = model(x_val)\n",
    "        loss_val += criterion(output_val, y_val).item()\n",
    "  \n",
    "    # Appending for plotting graph\n",
    "    val_losses.append(loss_val/1385)\n",
    "    train_losses.append(loss_train/4155)\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss_train/4155, epoch)\n",
    "    accuracy = accuracy_score(train_y.tolist(), predictions)\n",
    "    print('Epoch : ',epoch+1, '\\t', 'val loss :',loss_val/1385, '\\t', 'train loss :',loss_train/4155, '\\t',\"Accuracy: \", accuracy)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy  = accuracy\n",
    "        torch.save(model.state_dict(), \"/Users/ayushsharma/Documents/Programming/Python/dtd_custom\")\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "# np.save(\"train_losses.npy\", train_losses)\n",
    "# np.save(\"val_losses.npy\", val_losses)\n",
    "\n",
    "print(\"Training done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
